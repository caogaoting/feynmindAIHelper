import fitz  # PyMuPDF
import hashlib
from openai import OpenAI
from datetime import datetime
import os
from dotenv import load_dotenv
import re
import json
from tempfile import NamedTemporaryFile
import requests
from pathlib import Path
from pydantic import BaseModel
from typing import List
from concurrent.futures import ThreadPoolExecutor, as_completed
import tiktoken
load_dotenv()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL")
)

tokenizer = tiktoken.get_encoding("cl100k_base")

def trim_to_max_tokens(messages, max_tokens=65536):
    if not messages or len(messages) < 2:
        return messages

    system_msg = messages[0]
    user_msg = messages[1]

    # åˆ†å‰² user å†…å®¹ä¸ºè‹¥å¹²æ®µï¼ˆæŒ‰æ¢è¡Œåˆ†éš”çš„çŸ¥è¯†ç‚¹ï¼‰
    knowledge_list = user_msg["content"].split("\n")

    trimmed_knowledge = []
    total_tokens = len(tokenizer.encode(system_msg["content"]))
    
    for knowledge in knowledge_list:
        knowledge_tokens = len(tokenizer.encode(knowledge + "\n"))
        if total_tokens + knowledge_tokens > max_tokens:
            break
        trimmed_knowledge.append(knowledge)
        total_tokens += knowledge_tokens

    # è¿”å›è£å‰ªåçš„ messages
    return [
        system_msg,
        {"role": "user", "content": "\n".join(trimmed_knowledge)}
    ]

def append_and_trim_messages(messages, new_user_input, new_assistant_output=None, max_tokens=65536):
    """
    æ·»åŠ æ–°ä¸€è½®å¯¹è¯ï¼Œå¹¶è‡ªåŠ¨è£å‰ªè¶…å‡ºçš„å†å²å†…å®¹
    """
    if new_user_input:
        messages.append({"role": "user", "content": new_user_input})
    if new_assistant_output:
        messages.append({"role": "assistant", "content": new_assistant_output})

    # ä¿ç•™ system message
    system_msg = messages[0]
    history = messages[1:]

    # ä»åå¾€å‰ä¿ç•™æ¶ˆæ¯
    trimmed = []
    total_tokens = len(tokenizer.encode(system_msg["content"]))

    for msg in reversed(history):
        token_count = len(tokenizer.encode(msg["content"]))
        if total_tokens + token_count > max_tokens:
            break
        trimmed.insert(0, msg)
        total_tokens += token_count

    return [system_msg] + trimmed

def safe_json_dump(data):
    return json.dumps(data, ensure_ascii=False) if not isinstance(data, str) else data

def extract_pdf_text(path, max_pages=30):
    doc = fitz.open(path)
    text = ""
    for page in doc[:min(max_pages, len(doc))]:
        text += page.get_text()
    return text
    
def extract_json_from_text(text):
    # ç§»é™¤ Markdown ä»£ç å—åŒ…è£¹
    text = re.sub(r"```json|```", "", text).strip()
    json_start = text.find('[')
    json_end = text.rfind(']')
    if json_start != -1 and json_end != -1:
        return text[json_start:json_end + 1]
    return text
def extract_directory_text(pdf_path, directory_page):
    """
    æå–æŒ‡å®šç›®å½•é¡µçš„æ–‡æœ¬å†…å®¹ã€‚
    :param pdf_path: PDF æ–‡ä»¶è·¯å¾„
    :param directory_page: ç›®å½•æ‰€åœ¨é¡µæ•°ï¼ˆ1-basedï¼‰
    :return: ç›®å½•é¡µçš„æ–‡æœ¬å†…å®¹
    """
    doc = fitz.open(pdf_path)
    # è½¬æ¢ä¸º 0-based é¡µç 
    page = doc.load_page(directory_page - 1)
    return page.get_text()

def extract_text_in_range(pdf_path, start_page, end_page):
    """
    ä» PDF ä¸­æå–æŒ‡å®šé¡µç èŒƒå›´çš„æ–‡æœ¬ã€‚
    """
    doc = fitz.open(pdf_path)
    text = ""
    
    for page_num in range(start_page - 1, end_page):  # é¡µé¢æ˜¯ä» 0 å¼€å§‹çš„
        page = doc.load_page(page_num)
        text += page.get_text("text")  # æå–é¡µé¢æ–‡æœ¬
    return text

def analyze_pdf(filepath, mode="full", max_pages=1000, chapter_map=None):
    PDF_NAME = os.path.basename(filepath)
    BASE_DIR = Path("book_analysis")
    KNOWLEDGE_DIR = BASE_DIR / "knowledge_bases"
    SUMMARIES_DIR = BASE_DIR / "summaries"
    PDF_PATH = Path(filepath)
    OUTPUT_PATH = KNOWLEDGE_DIR / f"{PDF_NAME.replace('.pdf', '_knowledge.json')}"
    MODEL = "deepseek-chat"

    for directory in [KNOWLEDGE_DIR, SUMMARIES_DIR]:
        directory.mkdir(parents=True, exist_ok=True)

    doc = fitz.open(PDF_PATH)

    knowledge_base = defaultdict(list) if mode == "fast" else []

    def analyze_chapter(title, start, end):
        combined_text = f"ç« èŠ‚æ ‡é¢˜: {title}\nç¬¬ä¸€é¡µå†…å®¹:\n{doc[start].get_text()}\næœ€åä¸€é¡µå†…å®¹:\n{doc[end].get_text()}"
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ èµ„æ–™æå–åŠ©æ‰‹ï¼Œè¯·ä»ä»¥ä¸‹é¡µé¢ä¸­æå–å¯å­¦ä¹ çš„çŸ¥è¯†ç‚¹ï¼ˆå®šä¹‰ã€å…¬å¼ã€ç»“è®ºã€åŸç†ç­‰ï¼‰ï¼Œè·³è¿‡ç›®å½•ã€è‡´è°¢ã€ç‰ˆæƒé¡µã€ç´¢å¼•ç­‰æ— å®è´¨å†…å®¹é¡µé¢ã€‚

                    è¯·ä»¥ JSON ç»“æ„è¿”å›ï¼š
                    {
                    "has_content": true,
                    "knowledge": ["çŸ¥è¯†ç‚¹ä¸€...", "çŸ¥è¯†ç‚¹äºŒ..."]
                    }
                    å¦‚æ— å†…å®¹è¯·è¿”å›ï¼š
                    {
                    "has_content": false,
                    "knowledge": []
                    }"""}, 
                    {"role": "user", "content": combined_text}
                ],
                temperature=1.0
            )
            raw = response.choices[0].message.content.strip()
            raw = re.sub(r"^```json|```$", "", raw)
            parsed = json.loads(raw)
            return title, parsed if parsed.get("has_content") else None
        except Exception as e:
            print(f"âš ï¸ åˆ†æç« èŠ‚ {title} å¤±è´¥ï¼š{e}")
            return title, None

    def analyze_page(page_num):
        page_text = doc[page_num].get_text()
        if not page_text.strip():
            return page_num, None
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ èµ„æ–™æå–åŠ©æ‰‹ï¼Œè¯·ä»ä»¥ä¸‹é¡µé¢ä¸­æå–å¯å­¦ä¹ çš„çŸ¥è¯†ç‚¹ï¼ˆå®šä¹‰ã€å…¬å¼ã€ç»“è®ºã€åŸç†ç­‰ï¼‰ï¼Œè·³è¿‡ç›®å½•ã€è‡´è°¢ã€ç‰ˆæƒé¡µã€ç´¢å¼•ç­‰æ— å®è´¨å†…å®¹é¡µé¢ã€‚

                    è¯·ä»¥ JSON ç»“æ„è¿”å›ï¼š
                    {
                    "has_content": true,
                    "knowledge": ["çŸ¥è¯†ç‚¹ä¸€...", "çŸ¥è¯†ç‚¹äºŒ..."]
                    }
                    å¦‚æ— å†…å®¹è¯·è¿”å›ï¼š
                    {
                    "has_content": false,
                    "knowledge": []
                    }"""}, 
                    {"role": "user", "content": f"Page text: {page_text}"}
                ],
                temperature=1.0
            )
            raw = response.choices[0].message.content.strip()
            raw = re.sub(r"^```json|```$", "", raw)
            parsed = json.loads(raw)
            return page_num, parsed if parsed.get("has_content") else None
        except Exception as e:
            print(f"âš ï¸ åˆ†æç¬¬ {page_num+1} é¡µå¤±è´¥ï¼š{e}")
            return page_num, None

    if mode == "fast" and chapter_map:
        print("ğŸš€ å¹¶å‘ç« èŠ‚åˆ†æå¼€å§‹")
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            # æäº¤æ‰€æœ‰ç« èŠ‚åˆ†æä»»åŠ¡
            for chapter in chapter_map:
                start = chapter.get("start_page", 1) - 1
                end = chapter.get("end_page", 1) - 1
                if 0 <= start < doc.page_count and 0 <= end < doc.page_count:
                    title = chapter.get("title", f"ç« èŠ‚ {start+1}-{end+1}")
                    print(f"ğŸ“– æäº¤åˆ†æä»»åŠ¡: {title}")
                    futures.append(executor.submit(analyze_chapter, title, start, end))

            # ä¿è¯æŒ‰é¡ºåºå¤„ç†è¿”å›çš„ç»“æœ
            for future in futures:
                title, result = future.result()
                if result:
                    knowledge_base[title] = result["knowledge"]
                    print(f"âœ… æå– {title} çŸ¥è¯†ç‚¹ {len(result['knowledge'])} æ¡")
                else:
                    print(f"â­ï¸ {title} æ— å†…å®¹")

    elif mode == "full":
        print("ğŸš€ å¹¶å‘é€é¡µåˆ†æå¼€å§‹")
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            # æäº¤é€é¡µåˆ†æä»»åŠ¡
            for page_num in range(min(max_pages, doc.page_count)):
                futures.append(executor.submit(analyze_page, page_num))

            # ä¿è¯æŒ‰é¡ºåºå¤„ç†è¿”å›çš„ç»“æœ
            for future in futures:
                page_num, result = future.result()
                if result:
                    knowledge_base.extend(result["knowledge"])
                    print(f"âœ… ç¬¬ {page_num+1} é¡µæå–çŸ¥è¯†ç‚¹ {len(result['knowledge'])} æ¡")
                else:
                    print(f"â­ï¸ ç¬¬ {page_num+1} é¡µæ— å†…å®¹")

    else:
        print("âŒ å‚æ•°é”™è¯¯ï¼Œæœªæ‰§è¡Œåˆ†æ")
        return None

    # ä¿å­˜çŸ¥è¯†åº“
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump({"knowledge": knowledge_base}, f, indent=2, ensure_ascii=False)

    # ç»Ÿä¸€æ•´ç†
    if isinstance(knowledge_base, dict):
        all_knowledge = sum(knowledge_base.values(), [])
    else:
        all_knowledge = knowledge_base

    # ç”Ÿæˆæ€»ç»“
    print("ğŸ¤” æ­£åœ¨ç”Ÿæˆæ€»ç»“...")
    try:
        # åˆå§‹åŒ– messages
        messages = [
            {"role": "system", "content": "è¯·å°†ä»¥ä¸‹çŸ¥è¯†ç‚¹æ•´ç†ä¸ºç»“æ„åŒ– Markdown å­¦ä¹ æ€»ç»“ï¼Œä½¿ç”¨åˆç†çš„æ ‡é¢˜ä¸é¡¹ç›®ç¬¦å·åˆ—è¡¨"}
        ]

        # å°†çŸ¥è¯†ç‚¹æŒ‰æ®µåˆ†æ‰¹ï¼Œæ¯æ‰¹æ§åˆ¶é•¿åº¦
        def chunks(lst, n):
            for i in range(0, len(lst), n):
                yield lst[i:i + n]

        for knowledge_chunk in chunks(all_knowledge, 1000):
            user_input = "\n".join(knowledge_chunk)
            print(user_input)
            # å‘é€å½“å‰è½®çš„å¯¹è¯
            completion = client.chat.completions.create(
                model=MODEL,
                messages=messages + [{"role": "user", "content": user_input}],
                temperature=0.7
            )
            assistant_output = completion.choices[0].message.content.strip()

            # æ›´æ–°å¯¹è¯å†å²ï¼ˆå¹¶è‡ªåŠ¨è£å‰ªï¼‰
            messages = append_and_trim_messages(messages, user_input, assistant_output)

        # æœ€åä¸€è½®æ€»ç»“ç”Ÿæˆ
        final_completion = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            temperature=1.0
        )
        final_summary = final_completion.choices[0].message.content.strip()

        # ä¿å­˜ä¸º Markdown
        filename = f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        outpath = SUMMARIES_DIR / filename
        with open(outpath, 'w', encoding='utf-8') as f:
            f.write(f"# åˆ†ææ€»ç»“ï¼š{PDF_NAME}\n\n{final_summary}\n\n---\n*ç”± AI åˆ†æç”Ÿæˆ*")
        print(f"âœ… åˆ†æå®Œæˆï¼Œä¿å­˜äº {outpath}")
        return str(outpath)

    except Exception as e:
        print(f"âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼š{e}")
        return None


def analyze_chapters_by_ai_from_directory(pdf_path, start_page, end_page, project_id, db_save_fn,offset=0):
    doc = fitz.open(pdf_path)
    total_pages = len(doc)
    chapter_map = []

    fewshot_prompt = """
        è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­è¯†åˆ«å‡ºä¹¦ç±çš„ç« èŠ‚ç»“æ„ï¼Œå¹¶ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªç« èŠ‚å¯¹è±¡åº”åŒ…å«ï¼š
        - "title": ç« èŠ‚åç§°ï¼ˆå¦‚ï¼š"ç¬¬ä¸€ç«  ç»ªè®º"ï¼‰
        - "start_page": èµ·å§‹é¡µç ï¼ˆæ•´æ•°ï¼‰
        - "end_page": ç»“æŸé¡µç ï¼ˆæ•´æ•°ï¼‰

        è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š

        [
        { "title": "å¼•è¨€", "start_page": 1, "end_page": 2 },
        { "title": "ç¬¬ä¸€ç«  äººå·¥æ™ºèƒ½æ¦‚è¿°", "start_page": 3, "end_page": 10 },
        { "title": "ç¬¬äºŒç«  æ¨¡å‹è®­ç»ƒæ–¹æ³•", "start_page": 11, "end_page": 20 }
        ]

        è¯·ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–æ–‡å­—ã€è¯´æ˜æˆ–æ¢è¡Œåçš„æ³¨é‡Šï¼Œåªè¿”å› JSON æ ¼å¼çš„æ•°æ®ã€‚è¾“å‡ºæˆ‘ç»™ä½ çš„å†…å®¹ä¸­å¯æå–åˆ°çš„æ‰€æœ‰ç« èŠ‚ä¿¡æ¯ã€‚
        """

    chunk_text = ""
    for i in range(start_page - 1, end_page):  # -1 because PyMuPDF uses 0-based indexing
        if i < total_pages:
            text = doc.load_page(i).get_text()
            # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
            text = re.sub(r'\s+', '', text)  # ç§»é™¤ç©ºç™½å­—ç¬¦
            text = re.sub(r'[.,ï¼Œã€‚ã€ï¼›ï¼š""''ï¼ˆï¼‰()\[\]ã€ã€‘ï¼]', '', text)  # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            chunk_text += text
    print(f"ğŸ¤– æå–ç›®å½•é¡µå†…å®¹ï¼ˆå‰600å­—ï¼‰ï¼š{chunk_text[:600]}...")
    prompt = fewshot_prompt + "\n\n" + chunk_text[:10000]

    try:
        print(f"ğŸ¤– æ­£åœ¨åˆ†æç¬¬ {start_page} åˆ°ç¬¬ {end_page} é¡µç›®å½•ç»“æ„...")
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£ç»“æ„åˆ†æåŠ©æ‰‹ï¼Œè¯·æå–ä¹¦ç±ç« èŠ‚ç»“æ„ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=1,
        )
        ai_text = response.choices[0].message.content.strip()
        print(f"ğŸ¤– AI è¿”å›ï¼š\n{ai_text[:500]}...\n")
    except Exception as e:
        print(f"âš ï¸ åˆ†æå¤±è´¥ï¼šç¬¬ {start_page}-{end_page} é¡µï¼Œé”™è¯¯ï¼š{e}")
        return []

    try:
        cleaned_text = extract_json_from_text(ai_text)
        chapter_map = json.loads(cleaned_text)
    except Exception as parse_error:
        print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼š{parse_error}")
        return []
    # âœ… å¯¹æ‰€æœ‰ç« èŠ‚é¡µç åŠ ä¸Šåç½®
    for ch in chapter_map:
        ch["start_page"] += offset
        ch["end_page"] += offset
    # å»é‡ + æ’åº
    unique_map = {(c["title"], c["start_page"], c["end_page"]): c for c in chapter_map}
    final_map = sorted(unique_map.values(), key=lambda x: x["start_page"])

    for ch in final_map:
        section_text = ""
        for i in range(ch["start_page"] - 1, ch["end_page"]):
            if 0 <= i < total_pages:
                section_text += doc.load_page(i).get_text()

        section_summary = section_text[:500].replace("\n", " ")
        content_hash = hashlib.md5(section_text.encode("utf-8")).hexdigest()

        db_save_fn(
            project_id,
            ch["title"],
            ch["start_page"],
            ch["end_page"],
            section_summary,
            content_hash
        )

    print(f"âœ… å…±è¯†åˆ«ç« èŠ‚ {len(final_map)} ä¸ªã€‚")
    return final_map

def extract_structured_summary(full_summary):
    structure_prompt = f"""
        ä½ æ˜¯ä¸€åæ•™å­¦åŠ©ç†ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å­¦ä¹ èµ„æ–™æ€»ç»“ï¼Œæå–ä»¥ä¸‹ç»“æ„åŒ–å†…å®¹ï¼Œè¿”å› JSON æ ¼å¼ï¼š

        - classificationï¼šå¯¹ææ–™æ‰€å±å­¦ç§‘/ä¸»é¢˜çš„åˆ†ç±»
        - overviewï¼šå¯¹æ•´ä¸ªææ–™çš„ç®€è¦ä»‹ç»
        - outlineï¼šä¸ºææ–™æ„å»ºä¸€ä¸ªæ¡ç†æ¸…æ™°çš„å¤§çº²
        - questionsï¼šä»ææ–™ä¸­æå‡ºéœ€è¦è¿›ä¸€æ­¥ç†è§£æˆ–å°šæœªè§£å†³çš„é—®é¢˜ï¼ˆä¸å°‘äº3æ¡ï¼‰
        - keywordsï¼šæå–å­¦ä¹ èµ„æ–™ä¸­çš„å…³é”®è¯
        - main_sentencesï¼šæå–å­¦ä¹ èµ„æ–™ä¸­çš„ä¸»æ—¨å¥
        - argument_structureï¼šæå–å­¦ä¹ èµ„æ–™ä¸­çš„è®ºè¿°ç»“æ„
        - resolved_questionsï¼šåˆ—å‡ºå·²è§£å†³çš„é—®é¢˜
        - unresolved_questionsï¼šåˆ—å‡ºå¾…è§£å†³çš„é—®é¢˜

        è¯·ç”¨çº¯ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€markdownã€æ³¨é‡Šæˆ–å…¶ä»–å†…å®¹ã€‚

        ä»¥ä¸‹æ˜¯ææ–™æ€»ç»“å†…å®¹ï¼š
        {full_summary}
    """

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ•™å­¦åŠ©ç†ï¼Œæ“…é•¿æå–ç»“æ„åŒ–æ€»ç»“ä¿¡æ¯"},
            {"role": "user", "content": structure_prompt}
        ],
        temperature=1.0,
    )

    raw_output = response.choices[0].message.content
    print("ğŸ§  AI åŸå§‹è¿”å›å†…å®¹ï¼š\n", raw_output)

    try:
        # æå– markdown å—ä¸­çš„ JSON
        json_str = re.search(r"\{[\s\S]*\}", raw_output).group()
        struct_data = json.loads(json_str)
    except Exception as e:
        print("âŒ ç»“æ„åŒ–å­—æ®µæå–å¤±è´¥ï¼ŒåŸå› ï¼š", e)
        struct_data = {
            "classification": "",
            "overview": "",
            "outline": "",
            "questions": "",
            "keywords": "",
            "main_sentences": "",
            "argument_structure": "",
            "resolved_questions": "",
            "unresolved_questions": ""
        }

    return struct_data

def ask_question_in_section(pdf_path, start_page, end_page, question):
    """
    ä»æŒ‡å®šé¡µç èŒƒå›´æå–æ–‡æœ¬å¹¶æäº¤é—®é¢˜ç»™ AIã€‚
    """
    # æå–é¡µç èŒƒå›´å†…çš„æ–‡æœ¬
    text = extract_text_in_range(pdf_path, start_page, end_page)
    
    # å°†æ–‡æœ¬å’Œé—®é¢˜ä¸€èµ·æäº¤ç»™ AI è¿›è¡Œé—®ç­”
    try:
        response = client.chat.completions.create(
            model="deepseek-reasoner",  # ä½¿ç”¨ AI æ¨¡å‹
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ èµ„æ–™æå–åŠ©æ‰‹ï¼Œå¸®åŠ©å›ç­”å…³äºå­¦ä¹ èµ„æ–™çš„é—®é¢˜ã€‚"},
                {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ç›¸å…³å†…å®¹ï¼š\n{text}\n\né—®é¢˜ï¼š{question}\nè¯·åŸºäºè¿™äº›å†…å®¹å›ç­”é—®é¢˜ï¼š"}
            ],
            temperature=1.0
        )
        answer = response.choices[0].message.content.strip()
        return answer
    except Exception as e:
        print(f"âŒ æé—®å¤±è´¥ï¼š{e}")
        return "æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯ã€‚è¯·ç¨åå†è¯•ã€‚"
